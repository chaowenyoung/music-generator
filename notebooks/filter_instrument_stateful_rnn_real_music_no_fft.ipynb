{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Regeneration of lead synth from combined signal\"\"\"\n",
    "from tensorflow.keras.layers import Dense, Dropout, PReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from music_generator.basic.random import generate_dataset\n",
    "from music_generator.basic.signalproc import SamplingInfo\n",
    "from music_generator.musical.timing import Tempo\n",
    "from music_generator.musical.scales import GenericScale\n",
    "from music_generator.basic.signalproc import mix_at\n",
    "from music_generator.analysis import preprocessing\n",
    "\n",
    "from music_generator.musical import scales\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "from scipy.io.wavfile import read\n",
    "from music_generator.analysis import regen_models\n",
    "from scipy.io import wavfile\n",
    "from music_generator.analysis import jamdataset\n",
    "import tensorflow as tf\n",
    "\n",
    "from music_generator.analysis import regen_models\n",
    "import importlib\n",
    "\n",
    "from tensorflow.keras.layers import Input, GRU, PReLU, Dropout, Dense, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "\n",
    "from tensorflow.keras.callbacks import TensorBoard, ReduceLROnPlateau, ModelCheckpoint\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = 44100\n",
    "sampling_info = SamplingInfo(sr)\n",
    "fragment_length = 4096 * 5\n",
    "batch_size = 32\n",
    "sr = sampling_info.sample_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate in all keys\n",
    "all_roots = scales.chromatic_scale('C')\n",
    "roots = [n.get_symbol() for n in all_roots.generate(0, 1)]\n",
    "print(roots)\n",
    "\n",
    "def generate_dataset_for_root(root):\n",
    "    return generate_dataset(n_measures=32,\n",
    "                            tempo=Tempo(120),\n",
    "                            scale=GenericScale(root, [0, 2, 3, 5, 7, 8, 10]),\n",
    "                            sampling_info=sampling_info)\n",
    "\n",
    "def reshape_batches(x, batch_size, fragment_length):\n",
    "    \n",
    "    n_fragments = len(x) // fragment_length // batch_size\n",
    "    x = x[:n_fragments * fragment_length * batch_size].reshape(-1, fragment_length)\n",
    "    return x\n",
    "\n",
    "def process(dataset, batch_size, fragment_length):\n",
    "    # Make one big data set and make sure data is of same size        \n",
    "    audio_tracks, mix = preprocessing.combine_datasets(dataset)\n",
    "    \n",
    "    x, y = mix, audio_tracks[2]\n",
    "    \n",
    "    x = reshape_batches(x, batch_size, fragment_length)\n",
    "    y = reshape_batches(y, batch_size, fragment_length)    \n",
    "    \n",
    "    return x, y\n",
    "\n",
    "_, x = wavfile.read(\"../data/full-mix-jam1-01.wav\")\n",
    "_, y = wavfile.read(\"../data/only-guitar-jam1-01.wav\")\n",
    "\n",
    "x_train, y_train = reshape_batches(x / 2**15, batch_size, fragment_length), reshape_batches(y / 2**15, batch_size, fragment_length)\n",
    "    \n",
    "# with Pool(8) as pool:\n",
    "#     datasets_train = pool.map(generate_dataset_for_root, roots)\n",
    "#     datasets_test = pool.map(generate_dataset_for_root, roots)\n",
    "    \n",
    "# x_train, y_train = process(datasets_train, batch_size, fragment_length)\n",
    "# x_test, y_test = process(datasets_train, batch_size, fragment_length)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(x_train[18].reshape(-1), rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv1D?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_train = 4096\n",
    "# n_test = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio(input_track, rate=sr)\n",
    "# Audio(target_track, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, y = preprocessing.create_training_data_set(n_train + n_test, \n",
    "#                                               fragment_length, \n",
    "#                                               input_track, \n",
    "#                                               target_track)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(x_train[0], rate=sr)\n",
    "# Audio(y_train[0], rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_conv_model(batch_size, fragment_length):\n",
    "    \n",
    "    inp = Input(shape=[fragment_length])\n",
    "    \n",
    "    hidden = Reshape([fragment_length, 1])(inp)\n",
    "    \n",
    "    hidden = Conv1D(filters=20, kernel_size=5, padding='same')(hidden)\n",
    "    hidden = Conv1D(filters=20, kernel_size=5, padding='same')(hidden)\n",
    "    \n",
    "    hidden = Dense(1024, activation=\"relu\")(hidden)\n",
    "    hidden = Dense(512, activation=\"relu\")(hidden)\n",
    "    \n",
    "    hidden = Dense(1)(hidden)\n",
    "    hidden = Reshape([fragment_length])(hidden)\n",
    "    \n",
    "    out = hidden\n",
    "    \n",
    "    return Model(inp, out)\n",
    "    \n",
    "    \n",
    "def fft_loss(y_target, y_predicted):\n",
    "    y_target_complex = tf.cast(y_target, dtype=tf.complex64)\n",
    "    y_predicted_complex = tf.cast(y_predicted, dtype=tf.complex64)\n",
    "\n",
    "    loss = tf.square(tf.abs(tf.signal.fft(y_target_complex))[100:] -\n",
    "                     tf.abs(tf.signal.fft(y_predicted_complex))[100:])\n",
    "    return loss\n",
    "\n",
    "model = build_conv_model(batch_size, fragment_length)\n",
    "model.summary()\n",
    "model.compile(RMSprop(1e-3), loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = []\n",
    "\n",
    "# log_file_name = f\"tensorboard/{dt.datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "# tensorboard_callback = TensorBoard(log_dir=log_file_name, histogram_freq=1, update_freq='batch')\n",
    "# callbacks.append(tensorboard_callback)\n",
    "\n",
    "# reduce_lr_callback = ReduceLROnPlateau(verbose=1)\n",
    "# callbacks.append(reduce_lr_callback)\n",
    "\n",
    "# model_checkpoint_callback = ModelCheckpoint(\"weights.{epoch:02d}.h5\")\n",
    "# callbacks.append(model_checkpoint_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(model.predict(x_train[:64], verbose=1).reshape(-1), rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation_data=[x_test, y_test], "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, callbacks=callbacks, epochs=1, shuffle=True, batch_size=batch_size)\n",
    "Audio(model.predict(x_train[:batch_size*5], verbose=1).reshape(-1), rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_batches_inference = len(input_track) // fragment_length\n",
    "# inference_ds = input_track[:n_batches_inference * fragment_length]\n",
    "# inference_ds = inference_ds.reshape(-1, fragment_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_batches_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.predict(x_test, verbose=1).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_file, data = wavfile.read(\"../data/full-mix-jam1-01.wav\")\n",
    "assert sr_file == sr, \"Sample rate does not match, you will need to retrain\"\n",
    "data = reshape_batches(data, batch_size, fragment_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_2 = model.predict(data, verbose=1).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(data.reshape(-1)[:1000000], rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distortion due to phase matching issue?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = model.predict(x_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tmp.reshape(-1)[:1000])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
