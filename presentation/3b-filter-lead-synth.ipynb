{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, PReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from music_generator.prefabs.random_walk_track import generate_dataset\n",
    "from music_generator.signalproc.signalproc import SamplingInfo\n",
    "from music_generator.music.timing import Tempo\n",
    "from music_generator.music.scales import GenericScale\n",
    "from music_generator.signalproc.signalproc import mix_at\n",
    "from music_generator.analysis import preprocessing\n",
    "\n",
    "from music_generator.music import scales\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Filtering lead instrument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Goal\n",
    "\n",
    "We are going to generate some music with more than one synthesizer\n",
    "\n",
    "We will filter out the lead tone using a feed-forward neural network.\n",
    "\n",
    "```\n",
    "Model: input wave with 3 instruments -> output wave 1 instrument\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We will use a auto-encoder like setup. Replace the image by a short fragment of 1024 samples (~1/40th of a second) of sound data.\n",
    "\n",
    "<img src=\"images/ae.png\">\n",
    "\n",
    "[Image source](`https://medium.com/@curiousily/credit-card-fraud-detection-using-autoencoders-in-keras-tensorflow-for-hackers-part-vii-20e0c85301bd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Generating the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "sr = 44100\n",
    "sampling_info = SamplingInfo(sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Generate in all keys\n",
    "all_roots = scales.chromatic_scale('C')\n",
    "roots = [n.get_symbol() for n in all_roots.generate(0, 1)]\n",
    "print(roots)\n",
    "\n",
    "def generate_dataset_for_root(root):\n",
    "    return generate_dataset(n_measures=32,\n",
    "                            tempo=Tempo(120),\n",
    "                            scale=GenericScale(root, [0, 2, 3, 5, 7, 8, 10]),\n",
    "                            sampling_info=sampling_info)\n",
    "    \n",
    "with Pool(8) as pool:\n",
    "    datasets = pool.map(generate_dataset_for_root, roots)\n",
    "    \n",
    "# Make one big data set and make sure data is of same size    \n",
    "audio_tracks, mix = preprocessing.combine_datasets(datasets)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "mix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "audio_tracks.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "n_samples = 1024 * 40\n",
    "fragment_length = 1024 * 1\n",
    "input_track = mix\n",
    "target_track = audio_tracks[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Audio(input_track[0:8*sr], rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Audio(target_track[0:8*sr], rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img width='75%' src=\"images/ae.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Create training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "x, y = preprocessing.create_training_data_set(n_samples, \n",
    "                                              fragment_length, \n",
    "                                              input_track, \n",
    "                                              target_track)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "Audio(x[0], rate=sr)\n",
    "\n",
    "# play_array(np.tile(x[0], 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def x_fade_profile(batch_dim):\n",
    "    x = np.arange(batch_dim)\n",
    "    return 1 - abs(x - (batch_dim / 2)) / (batch_dim / 2)\n",
    "\n",
    "def model_predict(model, input_track):\n",
    "    dim = input_shape[0]\n",
    "    n_batches = int(len(input_track) / dim) - 1\n",
    "    pred_batches = input_track[0:n_batches*dim].reshape((-1, dim))\n",
    "    \n",
    "    pred_batches_shifted = input_track[dim//2:n_batches*dim + dim//2].reshape((-1, dim))\n",
    "    \n",
    "    xfp = x_fade_profile(dim)\n",
    "    \n",
    "    x0 = np.array([xfp * batch for batch in model.predict(pred_batches)]).reshape(-1)\n",
    "    x1 = np.array([xfp * batch for batch in model.predict(pred_batches_shifted)]).reshape(-1)\n",
    "    \n",
    "    return mix_at(x0, x1, dim//2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Time for some (deep) learning: build an auto-encoder-like network\n",
    "\n",
    "The model is just a simple feed forward neural network\n",
    "\n",
    "The architecture is one of a simple auto-encoder: same output dim as input dim. However, the data that we present is different: targets $\\neq$ inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<img src=\"images/ae.png\">\n",
    "\n",
    "[Image source](`https://medium.com/@curiousily/credit-card-fraud-detection-using-autoencoders-in-keras-tensorflow-for-hackers-part-vii-20e0c85301bd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "input_shape = x[0].shape\n",
    "output_shape = x[1].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(Dense(1024, input_shape=input_shape))\n",
    "model.add(PReLU())\n",
    "model.add(Dense(512))\n",
    "model.add(PReLU())\n",
    "model.add(Dense(output_shape))\n",
    "model.compile(Adam(), 'mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How does the network sound before training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Audio(model_predict(model, mix)[0:15*sr], rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fit the model in two epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.fit(x, y, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "display(Audio(mix[40*sr:45*sr], rate=sr))\n",
    "display(Audio(model_predict(model, mix)[40*sr:45*sr], rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Is it overfitted?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Of course it is overfitted on this particular sound of synth and backing track, and it will not work for any other sounds than this. But how well can it predict if we generate a completely new data set using a different scale (Phrygian Dominant, instead of minor)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "score_tracks_test, audio_tracks_test, mix_test = \\\n",
    "    generate_dataset(n_measures=64,\n",
    "                     tempo=Tempo(120),\n",
    "                     scale=GenericScale('E', [0, 1, 4, 5, 7, 8, 10]),\n",
    "                     sampling_info=sampling_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Audio(model_predict(model, mix_test[0:15*44100]), rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
