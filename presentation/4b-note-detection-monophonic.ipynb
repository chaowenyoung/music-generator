{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Input, GRU, Flatten, MaxPool2D, MaxPool1D\n",
    "from tensorflow.keras.layers import PReLU, Dropout, Lambda, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "from scipy.signal import stft\n",
    "\n",
    "from music_generator.basic.random import generate_dataset\n",
    "from music_generator.basic.signalproc import SamplingInfo\n",
    "from music_generator.synthesizer.instrument import make_lead_instrument, make_accomp_instrument\n",
    "from music_generator.musical.timing import Tempo, Duration\n",
    "from music_generator.musical.scales import GenericScale\n",
    "from music_generator.basic.signalproc import mix_at\n",
    "from music_generator.analysis import preprocessing\n",
    "from music_generator.musical.chords import ChordInScaleDefinition\n",
    "from music_generator.musical.notes import Note\n",
    "\n",
    "from music_generator.musical import scales\n",
    "from music_generator.musical import timing\n",
    "\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "\n",
    "from scipy.io.wavfile import read\n",
    "import pandas as pd\n",
    "\n",
    "from music_generator.basic import random\n",
    "from music_generator.synthesizer import instrument\n",
    "from music_generator.analysis import plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Goal\n",
    "\n",
    "We are going to generate some music and detect which notes were being played\n",
    "\n",
    "```\n",
    "Model: Wave in -> Sequence of notes out\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Generate random music\n",
    "2. Chop the wave data into small batches of a 1/10th of a second\n",
    "3. Use Fourier to go to a frequency representation of the sound\n",
    "4. For each of those batches, predict which note is being played\n",
    "5. Mold the data into a multiclass classification problem\n",
    "6. Each class corresponds to a note being played\n",
    "7. Train a model with a Gated Recurrent Unit as classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "sr = 44100\n",
    "stft_window_size = 4096\n",
    "sampling_info = SamplingInfo(sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generate & render the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "scale = scales.major_scale('C')\n",
    "tempo = timing.Tempo(120)\n",
    "signature = timing.Signature(4, 4)\n",
    "inst = instrument.make_lead_instrument(sampling_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def generate_dataset(scale, tempo, signature, inst, n_measures=128, n_notes_per_measure=16):\n",
    "    trk_lead = random.generate_lead_track(scale, \n",
    "                                          tempo, \n",
    "                                          signature, \n",
    "                                          n_measures=n_measures, \n",
    "                                          n_notes_per_measure=n_notes_per_measure)\n",
    "    audio = inst.generate_track(trk_lead)\n",
    "\n",
    "    df_targets = pd.DataFrame()\n",
    "    df_targets['offset'] = [x.offset.seconds for x in trk_lead.generate_notes()]\n",
    "    df_targets['duration'] = [x.duration.seconds for x in trk_lead.generate_notes()]\n",
    "    df_targets['end'] = df_targets['offset'] + df_targets['duration']\n",
    "    df_targets['note'] = [str(x.note) for x in trk_lead.generate_notes()]\n",
    "    df_targets = df_targets.groupby(['offset', 'end', 'note']).size().unstack('note').fillna(0).astype(int)\n",
    "    df_targets = df_targets.reset_index()\n",
    "    \n",
    "    return audio, trk_lead, df_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "audio, trk_lead, df_targets = generate_dataset(scale, tempo, signature, inst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multiclass note targets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df_targets.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "note_cols = [str(x) for x in \n",
    "             pd.Series([x.note for x in trk_lead.generate_notes()]).sort_values().drop_duplicates()]\n",
    "', '.join(note_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What does it sound like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "Audio(audio, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fourier analysis of the waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plots.set_style()\n",
    "plots.stft_plot(audio, sr, vmax=0.05)\n",
    "plt.xlim([0, 20])\n",
    "plt.ylim([0, 3000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Apply Fourier and collect the right targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def clip_incomplete_batch(arr, batch_size):\n",
    "    return arr[0:(len(arr) // batch_size) * batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(audio, df_targets, n_steps, batch_size):\n",
    "    \n",
    "    # Apply Fourier on the waveform\n",
    "    stft_frequencies, stft_timesteps, stft_transformed = stft(audio, sr, nperseg=stft_window_size)\n",
    "\n",
    "    X_train = np.abs(stft_transformed)\n",
    "    X_train = X_train.T\n",
    "    X_train = clip_incomplete_batch(X_train, n_steps)\n",
    "    X_train = X_train.reshape(X_train.shape[0] // n_steps, n_steps, X_train.shape[1])\n",
    "    \n",
    "    # Based on time t, we collect the targets\n",
    "    y_train = np.array([df_targets[(df_targets.offset <= t) & \n",
    "                                   (t < df_targets.end)][note_cols].sum().values for t in stft_timesteps])\n",
    "    \n",
    "    # Clipping or zero-padding is needed in order for the dimensions to be OK for training\n",
    "    X_train = clip_incomplete_batch(X_train, batch_size)\n",
    "    y_train = clip_incomplete_batch(y_train, batch_size)\n",
    "    \n",
    "    return X_train, y_train, stft_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "n_steps = 1\n",
    "batch_size = 32\n",
    "\n",
    "X_train, y_train, stft_frequencies = preprocess(audio, df_targets, n_steps, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Why Fourier is helping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "sel = np.arange(0, 200)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "\n",
    "plt.sca(ax[0])\n",
    "sel = np.arange(0, 200)\n",
    "plt.pcolormesh(X_train[sel, 0, 0:130].T, vmin=0, vmax=0.05)\n",
    "\n",
    "plt.sca(ax[1])\n",
    "sel = np.arange(0, 200)\n",
    "plt.pcolormesh(y_train[sel].T, vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Setting up the GRU\n",
    "\n",
    "### Layer Design\n",
    "\n",
    "* The hidden state is 14 dimensional, which is also the number of different notes that can be played\n",
    "* The number of timesteps is set to only 1, but the RNN is stateful\n",
    "* Each timestep corresponds to a Fourier spectrum, which is a vertical slice in the FFT plot above\n",
    "* The `input_dim` parameter corresponds to the number of frequencies in FFT (=2048 + 1, in this case)\n",
    "* Note that in the time-domain this corresponds to 4096 samples, i.e. ~0.1 seconds\n",
    "* For each of the timesteps a note is being predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "n_channels = stft_frequencies.shape[0]\n",
    "inp = Input(shape=(n_steps, n_channels)) # , batch_shape=(batch_size,n_steps, n_channels))\n",
    "\n",
    "gru = GRU(14)\n",
    "dense = Dense(len(note_cols), activation='sigmoid')\n",
    "\n",
    "x = inp\n",
    "x = gru(x)\n",
    "out = dense(x)\n",
    "\n",
    "model = Model(inp, out)\n",
    "model.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=16, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_train, batch_size=batch_size)\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "sel = np.arange(0, 100)\n",
    "\n",
    "plt.sca(ax[0])\n",
    "plt.pcolormesh(y_pred[sel].T, vmin=0, vmax=1)\n",
    "plt.sca(ax[1])\n",
    "plt.pcolormesh(y_train[sel].T, vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we continue training for one night we can get a very good performance on this data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "audio, trk_lead, df_targets = generate_dataset(scale, tempo, signature, inst)\n",
    "X_test, y_test, stft_frequencies = preprocess(audio, df_targets, n_steps, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test, batch_size=batch_size)\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "sel = np.arange(0, 100)\n",
    "\n",
    "plt.sca(ax[0])\n",
    "plt.pcolormesh(y_pred[sel].T, vmin=0, vmax=1)\n",
    "plt.sca(ax[1])\n",
    "plt.pcolormesh(y_test[sel].T, vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
