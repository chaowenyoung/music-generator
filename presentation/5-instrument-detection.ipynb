{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Real music example\n",
    "\n",
    "## Detect when an instrument is playing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Input, GRU, Flatten, MaxPool2D, MaxPool1D\n",
    "from tensorflow.keras.layers import PReLU, Dropout, Lambda, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "from music_generator.basic.random import generate_dataset\n",
    "from music_generator.basic.signalproc import SamplingInfo\n",
    "from music_generator.musical.timing import Tempo\n",
    "from music_generator.musical.scales import GenericScale\n",
    "from music_generator.basic.signalproc import mix_at\n",
    "from music_generator.analysis import preprocessing\n",
    "\n",
    "from music_generator.musical import scales\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "\n",
    "from scipy.io.wavfile import read\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = (16.0, 12.0)\n",
    "matplotlib.rcParams['lines.linewidth'] = 2\n",
    "matplotlib.rcParams['axes.linewidth'] = 1.5\n",
    "matplotlib.rcParams['font.size'] = 18\n",
    "matplotlib.rcParams['xtick.major.size'] = 5\n",
    "matplotlib.rcParams['xtick.major.width'] = 2\n",
    "matplotlib.rcParams['ytick.major.size'] = 5\n",
    "matplotlib.rcParams['ytick.major.width'] = 2\n",
    "matplotlib.rcParams['figure.figsize'] = (16.0, 8.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "sr, full_mix = read(\"../data/full-mix.wav\", mmap=False)\n",
    "sr, only_guitar = read(\"../data/only-guitar.wav\", mmap=False)\n",
    "\n",
    "full_mix = full_mix.astype(np.float) / 2**15\n",
    "only_guitar = only_guitar.astype(np.float) / 2**15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Visualize the full mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.signal import stft\n",
    "f_vec, t_vec, Zxx = stft(full_mix, sr, nperseg=4096)\n",
    "plt.pcolormesh(t_vec, f_vec, np.abs(Zxx), vmin=0, vmax=0.5)\n",
    "# plt.title('STFT Magnitude')\n",
    "plt.ylabel('Frequency [Hz]')\n",
    "plt.xlabel('Time [sec]')\n",
    "plt.ylim(0, 3000)\n",
    "plt.xlim(30, 40);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Detect where guitar is playing in its solo track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'guitar': only_guitar}, index=np.arange(len(only_guitar))/ 44100)\n",
    "df['full_mix'] = full_mix\n",
    "df['rolling_mse'] = df['guitar'].rolling(window=4000, center=False).std()\n",
    "df['is_playing'] = df['rolling_mse'] > 0.1\n",
    "df['is_playing_float'] = df['is_playing'] * 1.0\n",
    "df['signal_masked'] = df['is_playing'] * df['guitar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "t0 = 40\n",
    "delta = 10\n",
    "df.loc[t0:t0+delta][['guitar', 'rolling_mse', 'is_playing_float']].plot()\n",
    "df.loc[t0:t0+delta]['full_mix'].plot(color='0.5', alpha=0.3)\n",
    "plt.axhline(y=0, color='k')\n",
    "plt.ylabel('amp')\n",
    "plt.xlabel('time [sec]');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "Audio(df['signal_masked'].values[20*sr:30*sr], rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Make data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def create_random_fragment_indices(n_batches, n_samples, fragment_length):\n",
    "    i_max = n_samples - fragment_length\n",
    "    start = np.random.randint(low=0, high=i_max, size=n_batches)\n",
    "    \n",
    "    return np.array([np.arange(s, s+fragment_length) for s in start])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "x_train = df['full_mix'].values\n",
    "y_train = df['is_playing'].values\n",
    "\n",
    "fragment_length = 4096\n",
    "n_fragments = 20000\n",
    "\n",
    "ind = create_random_fragment_indices(n_fragments, len(x_train), fragment_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "x_train = x_train[ind]\n",
    "y_train = y_train[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(-1, fragment_length)\n",
    "y_train = y_train.reshape(-1, fragment_length)\n",
    "\n",
    "y_train = np.percentile(y_train, q=80, axis=1).reshape(-1, 1)\n",
    "\n",
    "# Class balancing\n",
    "w0 = 0.5*len(x_train) / np.sum(y_train == 0) \n",
    "w1 = 0.5*len(x_train) / np.sum(y_train == 1)\n",
    "sample_weight = np.where(y_train==0, w0, w1).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Build model\n",
    "\n",
    "* We are going to chop up again in fragments of 0.1 seconds (which is the time resolution)\n",
    "* For each fragment we are know whether guitar is playing or not\n",
    "\n",
    "    ```\n",
    "    Short fragment (~0.1 seconds) -> Model -> is_playing\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "inp = Input(shape=[fragment_length])\n",
    "out = inp\n",
    "\n",
    "def apply_fft(tensor):\n",
    "    tensor = tf.cast(tensor, tf.complex64)\n",
    "    return tf.abs(tf.signal.fft(tensor))\n",
    "\n",
    "out = Lambda(apply_fft, output_shape=[fragment_length])(out)\n",
    "out = Dense(units=1, activation='sigmoid')(out)\n",
    "\n",
    "model = Model(inp, out)\n",
    "model.summary()\n",
    "model.compile(Adam(lr=1e-3), 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, epochs=10, validation_split=0.2, sample_weight=sample_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "df = df.iloc[0:((len(df) // fragment_length) * fragment_length)].copy()\n",
    "x_pred = df.full_mix.values.reshape(-1, fragment_length)\n",
    "y_pred = model.predict(x_pred)\n",
    "df['is_playing_prediction'] = y_pred.repeat(fragment_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Visualize prediction using moviepy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import moviepy as mpy\n",
    "from moviepy.editor import VideoClip, AudioFileClip, clips_array\n",
    "from moviepy.video.io.bindings import mplfig_to_npimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = (8.0, 4.0)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "def make_frame1(t):\n",
    "    ax.clear()\n",
    "    ix = df.index.searchsorted(t)\n",
    "    c = '0.5' if df.iloc[ix]['is_playing_prediction'] < 0.5 else 'b'\n",
    "    ax.plot(df.loc[t-0.2:t]['full_mix'].values, color=c)\n",
    "    ax.set_ylim(-2, 2)\n",
    "    return mplfig_to_npimage(fig)\n",
    "\n",
    "def make_frame2(t):\n",
    "    ax.clear()\n",
    "    ix = df.index.searchsorted(t)\n",
    "    c = '0.5' if df.iloc[ix]['is_playing_prediction'] < 0.5 else 'b'\n",
    "    ax.plot(df.loc[t-0.2:t]['guitar'].values, color=c)\n",
    "    ax.set_ylim(-2, 2)\n",
    "    return mplfig_to_npimage(fig)\n",
    "\n",
    "def make_frame3(t):\n",
    "    ax.clear()\n",
    "    ax.plot(df.loc[t-1:t]['is_playing'].values)\n",
    "    ax.plot(df.loc[t-1:t]['is_playing_prediction'].values)\n",
    "    ax.set_ylim(-0.5, 1.5)\n",
    "    return mplfig_to_npimage(fig)\n",
    "\n",
    "duration = df.index.max()\n",
    "# duration = 10\n",
    "\n",
    "clip1 = VideoClip(make_frame1, duration=duration)\n",
    "clip2 = VideoClip(make_frame2, duration=duration)\n",
    "clip3 = VideoClip(make_frame3, duration=duration)\n",
    "\n",
    "audio = AudioFileClip(\"../data/full-mix.wav\", fps=44100)\n",
    "clip1.audio = audio\n",
    "\n",
    "final_clip = clips_array([[clip1, clip3]])\n",
    "# final_clip.ipython_display(fps=20, autoplay=True, maxduration=300)\n",
    "final_clip.to_videofile(\"detection.mp4\", fps=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's listen and watch the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "Video('detection.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
